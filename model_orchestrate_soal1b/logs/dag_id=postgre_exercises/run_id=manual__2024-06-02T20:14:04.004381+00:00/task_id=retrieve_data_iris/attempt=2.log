[2024-06-02T20:18:30.285+0000] {taskinstance.py:1979} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: postgre_exercises.retrieve_data_iris manual__2024-06-02T20:14:04.004381+00:00 [queued]>
[2024-06-02T20:18:30.288+0000] {taskinstance.py:1979} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: postgre_exercises.retrieve_data_iris manual__2024-06-02T20:14:04.004381+00:00 [queued]>
[2024-06-02T20:18:30.288+0000] {taskinstance.py:2193} INFO - Starting attempt 2 of 3
[2024-06-02T20:18:30.296+0000] {taskinstance.py:2214} INFO - Executing <Task(PythonOperator): retrieve_data_iris> on 2024-06-02 20:14:04.004381+00:00
[2024-06-02T20:18:30.300+0000] {standard_task_runner.py:60} INFO - Started process 1747 to run task
[2024-06-02T20:18:30.303+0000] {standard_task_runner.py:87} INFO - Running: ['***', 'tasks', 'run', 'postgre_exercises', 'retrieve_data_iris', 'manual__2024-06-02T20:14:04.004381+00:00', '--job-id', '21', '--raw', '--subdir', 'DAGS_FOLDER/create_table.py', '--cfg-path', '/tmp/tmp71ucma45']
[2024-06-02T20:18:30.304+0000] {standard_task_runner.py:88} INFO - Job 21: Subtask retrieve_data_iris
[2024-06-02T20:18:30.338+0000] {task_command.py:423} INFO - Running <TaskInstance: postgre_exercises.retrieve_data_iris manual__2024-06-02T20:14:04.004381+00:00 [running]> on host 35a5c0dbc0eb
[2024-06-02T20:18:30.380+0000] {taskinstance.py:2510} INFO - Exporting env vars: AIRFLOW_CTX_DAG_EMAIL='msaipulrx@gmail.com' AIRFLOW_CTX_DAG_OWNER='saipul' AIRFLOW_CTX_DAG_ID='postgre_exercises' AIRFLOW_CTX_TASK_ID='retrieve_data_iris' AIRFLOW_CTX_EXECUTION_DATE='2024-06-02T20:14:04.004381+00:00' AIRFLOW_CTX_TRY_NUMBER='2' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-06-02T20:14:04.004381+00:00'
[2024-06-02T20:18:30.420+0000] {logging_mixin.py:188} INFO - Table Input
[2024-06-02T20:18:30.432+0000] {logging_mixin.py:188} INFO -     ?column?       ?column?      ?column?       ?column?      ?column?
0         Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm
1         Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm
2         Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm
3         Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm
4         Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm
..       ...            ...           ...            ...           ...
145       Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm
146       Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm
147       Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm
148       Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm
149       Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm

[150 rows x 5 columns]
[2024-06-02T20:18:30.432+0000] {python.py:202} INFO - Done. Returned value was:     ?column?       ?column?      ?column?       ?column?      ?column?
0         Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm
1         Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm
2         Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm
3         Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm
4         Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm
..       ...            ...           ...            ...           ...
145       Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm
146       Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm
147       Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm
148       Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm
149       Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm

[150 rows x 5 columns]
[2024-06-02T20:18:30.470+0000] {xcom.py:664} ERROR - Duplicate column names found: ['?column?', '?column?', '?column?', '?column?', '?column?']. If you are using pickle instead of JSON for XCom, then you need to enable pickle support for XCom in your *** config or make sure to decorate your object with attr.
[2024-06-02T20:18:30.471+0000] {taskinstance.py:2728} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 451, in _execute_task
    task_instance.xcom_push(key=XCOM_RETURN_KEY, value=xcom_value, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 3010, in xcom_push
    XCom.set(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/xcom.py", line 247, in set
    value = cls.serialize_value(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/xcom.py", line 662, in serialize_value
    return json.dumps(value, cls=XComEncoder).encode("UTF-8")
  File "/usr/local/lib/python3.8/json/__init__.py", line 234, in dumps
    return cls(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/json.py", line 104, in encode
    return super().encode(o)
  File "/usr/local/lib/python3.8/json/encoder.py", line 199, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File "/usr/local/lib/python3.8/json/encoder.py", line 257, in iterencode
    return _iterencode(o, 0)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/json.py", line 91, in default
    return serialize(o)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/serialization/serde.py", line 148, in serialize
    data, serialized_classname, version, is_serialized = _serializers[qn].serialize(o)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/serialization/serializers/pandas.py", line 49, in serialize
    table = pa.Table.from_pandas(o)
  File "pyarrow/table.pxi", line 3869, in pyarrow.lib.Table.from_pandas
  File "/home/airflow/.local/lib/python3.8/site-packages/pyarrow/pandas_compat.py", line 572, in dataframe_to_arrays
    convert_fields) = _get_columns_to_convert(df, schema, preserve_index,
  File "/home/airflow/.local/lib/python3.8/site-packages/pyarrow/pandas_compat.py", line 354, in _get_columns_to_convert
    raise ValueError(
ValueError: Duplicate column names found: ['?column?', '?column?', '?column?', '?column?', '?column?']
[2024-06-02T20:18:30.481+0000] {taskinstance.py:1149} INFO - Marking task as UP_FOR_RETRY. dag_id=postgre_exercises, task_id=retrieve_data_iris, execution_date=20240602T201404, start_date=20240602T201830, end_date=20240602T201830
[2024-06-02T20:18:30.489+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 21 for task retrieve_data_iris (Duplicate column names found: ['?column?', '?column?', '?column?', '?column?', '?column?']; 1747)
[2024-06-02T20:18:30.525+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2024-06-02T20:18:30.552+0000] {taskinstance.py:3309} INFO - 0 downstream tasks scheduled from follow-on schedule check
